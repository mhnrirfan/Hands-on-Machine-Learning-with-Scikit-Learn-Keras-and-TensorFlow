{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhnrirfan/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/Missing_values%2C_normalisation%2C_classifier_evaluation_(with_solutions).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddTtq6yieoXw"
      },
      "source": [
        "# Missing values, normalisation, classifier evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6gVuZV-eoXx"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYau9UaVeoXy"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Given dataset (as an Numpy array) that contains  records about 50 persons: (Gender, Height, Weight, Index).\n",
        "- Gender = 0 (Male); 1 (Female) (binary)\n",
        "- Height = person's height in cm (integer)\n",
        "- Weight = person's weight in kg (integer)\n",
        "- Index = 0 - Extremely Weak; 1 - Weak; 2 - Normal; 3 - Overweight; 4 - Obesity; 5 - Extreme Obesity (integer from 0 to 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmSErwFyeoXz"
      },
      "outputs": [],
      "source": [
        "dataset = np.array([[0, 174, -1, 4], [0, 189, 87, 2], [1, 185, 110, 4], [1, 195, 104, 3], [0, 149, 61, 3], [0, 1893, 104, 3], [0, 147, 92, 5], [0, 154, 111, 5], [0, 174, 2, 3], [10, 169, 103, 4], [0, 195, 81, 2], [-1, 159, 80, 4], [1, 192, 101, 3], [0, 155, 51, 2], [0, 191, 379, 2], [1, 4, 107, 5], [1, 157, 110, 5], [0, 140, 129, 5], [0, 144, 145, 5], [0, 172, 139, -5], [0, 157, 110, 5], [1, 153, 149, 5], [1, 169, 97, 4], [0, 185, 139, 5], [1, 172, 67, 2], [1, 151, 64, 3], [0, 190, 95, 3], [0, 187, 62, 1], [1, 163, 159, 5], [0, 179, 152, 5], [0, 153, 121, 5], [0, 178, 52, 1], [1, 195, 65, 1], [1, 160, 131, 5], [1, 157, 153, 5], [1, 189, 132, 4], [1, 197, 114, 3], [0, 144, 80, 4], [1, 171, 152, 5], [1, 185, 81, 2], [1, 175, 120, 4], [1, 149, 108, 5], [0, 157, 56, 2], [0, 161, 118, 5], [1, 182, 126, 4], [0, 185, 76, 2], [1, 188, 122, 4], [0, 181, 111, 4], [0, 161, 72, 3], [0, 140, 152, 5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv3cIzDseoXz"
      },
      "source": [
        "1. Output the records with noisy data, i.e., records where values of some features are most likely incorrect (you can assume that Height should be in the range $[50,220]$, and Weight in the range $[50, 160]$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaVT0RyVeoXz"
      },
      "outputs": [],
      "source": [
        "# First we introduce a function that validates an input record.\n",
        "# The function returns True if the record seems to be corrupted/noisy\n",
        "def isNoisyRecord(record):\n",
        "    # Check if all fileds (features) of the record satisfies 'reasonable' assumptions\n",
        "    if (record[0] in [0, 1]) and (record[1] in range(50, 221)) and (record[2] in range(50, 160)) and (record[3] in range(0, 6)):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "# Read the number of records in the dataset\n",
        "numRecords = dataset.shape[0]\n",
        "\n",
        "# Go through the records one by one and print potentially noisy records\n",
        "print(\"Noisy records\")\n",
        "for recordIndex in range(numRecords):\n",
        "    if isNoisyRecord(dataset[recordIndex]) == True:\n",
        "        print(str(recordIndex+1)+\".\", repr(dataset[recordIndex]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SCyJghNeoX0"
      },
      "source": [
        "2. Create a new dataset where noisy records (those that were identified as noisy in the previous step) are removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG8RF6IQeoX0"
      },
      "outputs": [],
      "source": [
        "# Copy original dataset to a new variable\n",
        "cleanDataset = np.empty((0,4), int)\n",
        "\n",
        "# Copy to the new dataset non-noisy records\n",
        "for recordIndex in range(numRecords):\n",
        "    if isNoisyRecord(dataset[recordIndex]) == False:\n",
        "        cleanDataset = np.vstack((cleanDataset, dataset[recordIndex]))\n",
        "\n",
        "print(cleanDataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPj3Z7rneoX0"
      },
      "source": [
        "3. In the cleaned dataset, compute the means (i.e., average) of Height and Weight features (rounded to the nearest integer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWlourX0eoX0"
      },
      "outputs": [],
      "source": [
        "meanHeight = int(round(np.mean(cleanDataset[:,1], axis=0)))\n",
        "meanWeight = int(round(np.mean(cleanDataset[:,2], axis=0)))\n",
        "\n",
        "print(\"The mean of Height: \", meanHeight)\n",
        "print(\"The mean of Weight: \", meanWeight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGm3PK70eoX0"
      },
      "source": [
        "4. Plot the data points from the cleaned dataset: For every record, plot the point with coordinates (Height, Weight), i.e. Height is along the X-axis and Weight is along the Y-axis. Use different colors/shapes of points for records with different value of Index. Can you see separation between the classes of objects with different values of Index?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROjhwUFAeoX0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Select records with specific Index\n",
        "mask = (cleanDataset[:,3] == 0)\n",
        "cleanDatasetIndex0 = cleanDataset[mask,:]\n",
        "\n",
        "mask = (cleanDataset[:,3] == 1)\n",
        "cleanDatasetIndex1 = cleanDataset[mask,:]\n",
        "\n",
        "mask = (cleanDataset[:,3] == 2)\n",
        "cleanDatasetIndex2 = cleanDataset[mask,:]\n",
        "\n",
        "mask = (cleanDataset[:,3] == 3)\n",
        "cleanDatasetIndex3 = cleanDataset[mask,:]\n",
        "\n",
        "mask = (cleanDataset[:,3] == 4)\n",
        "cleanDatasetIndex4 = cleanDataset[mask,:]\n",
        "\n",
        "mask = (cleanDataset[:,3] == 5)\n",
        "cleanDatasetIndex5 = cleanDataset[mask,:]\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(cleanDatasetIndex0[:,1],cleanDatasetIndex0[:,2], marker='o', color='r')\n",
        "plt.scatter(cleanDatasetIndex1[:,1],cleanDatasetIndex1[:,2], marker='o', color='g')\n",
        "plt.scatter(cleanDatasetIndex2[:,1],cleanDatasetIndex2[:,2], marker='o', color='b')\n",
        "plt.scatter(cleanDatasetIndex3[:,1],cleanDatasetIndex3[:,2], marker='o', color='c')\n",
        "plt.scatter(cleanDatasetIndex4[:,1],cleanDatasetIndex4[:,2], marker='o', color='m')\n",
        "plt.scatter(cleanDatasetIndex5[:,1],cleanDatasetIndex5[:,2], marker='o', color='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA8O0oBceoX0"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2\n",
        "\n",
        "Given a dataset of objects with 2 features, create new datasets where both features are normalised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwQbDMcCeoX1"
      },
      "outputs": [],
      "source": [
        "dataset2 = np.array([[116.87714213,  22.38724318],[ 98.05089468,  17.70419866],[121.91732905,  20.76950035],[150.60375643,  20.44777321],[116.15969721,  20.22792041],[175.55264595,  21.02920868],[130.2102414 ,  22.23461658],[117.79758934,  21.54449521],[199.05450964,  20.53902401],[155.00458477,  20.67588878],[160.27851623,  20.30877322],[142.49587465,  18.75387901],[167.32252309,  21.2040159 ],[178.11489609,  19.52285308],[135.76602655,  18.01816954],[117.62628615,  20.95436217],[171.74626081,  20.37269529],[130.11860385,  19.56387712],[122.68044125,  21.48927022],[159.43238282,  21.38030275],[108.78123817,  19.86203164],[152.70729255,  21.24708246],[143.64358057,  21.40357256],[209.68785285,  19.65378059],[133.08486628,  19.23372601],[138.2204662 ,  20.37983756],[143.65610499,  19.92235183],[143.29146765,  20.56157279],[106.79099845,  19.35181412],[208.89304694,  19.9697894 ],[183.55451445,  20.36281171],[144.46962995,  19.83011097],[174.87583233,  20.25079178],[137.33207546,  21.02909898],[187.74834101,  20.23883521],[127.59840561,  21.7328862 ],[110.15966458,  19.29460209],[130.82208863,  18.98107528],[166.72373008,  20.63258552],[154.50866392,  21.05583445],[190.93412702,  21.08776453],[148.62481651,  19.40248102],[115.50503568,  18.39916603],[155.39233722,  20.23610734],[146.44242825,  20.30917973],[114.21209804,  19.10945672],[140.4159023 ,  19.96369514],[140.82108456,  20.05603083],[129.50278428,  18.92210224],[174.88635445,  19.03980704]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6EFtaXeoX1"
      },
      "source": [
        "1. Normalise features with $[0,1]$-scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmefrLzueoX1"
      },
      "outputs": [],
      "source": [
        "dataset2_01normalised = dataset2.copy()\n",
        "\n",
        "feature1max = dataset2[:,0].max(axis=0)\n",
        "feature1min = dataset2[:,0].min(axis=0)\n",
        "\n",
        "feature2max = dataset2[:,1].max(axis=0)\n",
        "feature2min = dataset2[:,1].min(axis=0)\n",
        "\n",
        "#[0,1]-normalisation of the first feature\n",
        "dataset2_01normalised[:,0] = (dataset2[:,0] - feature1min)/(feature1max - feature1min)\n",
        "\n",
        "#[0,1]-normalisation of the second feature\n",
        "dataset2_01normalised[:,1] = (dataset2[:,1] - feature2min)/(feature2max - feature2min)\n",
        "\n",
        "print(dataset2_01normalised)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZeGXeVxeoX1"
      },
      "source": [
        "2. Normalise features with Gaussian normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWqVR0ngeoX1"
      },
      "outputs": [],
      "source": [
        "dataset2_GausssianNormalised = dataset2.copy()\n",
        "\n",
        "feature1mean = dataset2[:,0].mean(axis=0)\n",
        "feature1std = dataset2[:,0].std(axis=0)\n",
        "\n",
        "feature2mean = dataset2[:,1].mean(axis=0)\n",
        "feature2std = dataset2[:,1].std(axis=0)\n",
        "\n",
        "#Gaussian normalisation of the first feature\n",
        "dataset2_GausssianNormalised[:,0] = (dataset2[:,0] - feature1mean)/feature1std\n",
        "\n",
        "#Gaussian normalisation of the second feature\n",
        "dataset2_GausssianNormalised[:,1] = (dataset2[:,1] - feature2mean)/feature2std\n",
        "\n",
        "\n",
        "print(dataset2_GausssianNormalised)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpO6mjYpeoX1"
      },
      "source": [
        "3. In three different plots show the original data, $[0,1]$-normilised, and Gaussian normalised data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtc5n_keeoX1"
      },
      "outputs": [],
      "source": [
        "plt.scatter(dataset2[:,0],dataset2[:,1], marker='o', color='b')\n",
        "plt.title('Original dataset')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(dataset2_01normalised[:,0],dataset2_01normalised[:,1], marker='o', color='r')\n",
        "plt.title('[0,1]-normalised dataset')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(dataset2_GausssianNormalised[:,0],dataset2_GausssianNormalised[:,1], marker='o', color='g')\n",
        "plt.title('Gausssian normalised dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x98YEUlEeoX1"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 3\n",
        "\n",
        "Given training ($\\texttt{y_true}$) and predicted ($\\texttt{y_pred}$) data for binary classification, where 1 corresponds to the positive class and 0 corresponds to the negative class, compute\n",
        "1. The number of True Positives, True Negatives, False Positives, False Negatives\n",
        "2. Accuracy\n",
        "3. Precision\n",
        "4. Recall\n",
        "5. F-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYDmCyWReoX1"
      },
      "outputs": [],
      "source": [
        "y_true = np.array([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])\n",
        "y_pred = np.array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBv4eniOeoX1"
      },
      "outputs": [],
      "source": [
        "positive_mask = y_true == 1\n",
        "\n",
        "# Count the number of elements in the positive class\n",
        "positive = np.count_nonzero(positive_mask)\n",
        "# Count True Positive\n",
        "tp = np.count_nonzero(y_pred[positive_mask]==1)\n",
        "# Count False Negative\n",
        "fn = np.count_nonzero(y_pred[positive_mask]==0)\n",
        "\n",
        "print(\"Num. of Positive:\", positive)\n",
        "print(\"Num. of True Positive:\", tp)\n",
        "print(\"Num. of False Negative:\", fn)\n",
        "print(\"\\n\")\n",
        "\n",
        "negative_mask = y_true == 0\n",
        "\n",
        "# Count the number of elements in the negative class\n",
        "negative = np.count_nonzero(negative_mask)\n",
        "# Count False Positive\n",
        "fp = np.count_nonzero(y_pred[negative_mask]==1)\n",
        "# Count True Negative\n",
        "tn = np.count_nonzero(y_pred[negative_mask]==0)\n",
        "\n",
        "print(\"Num. of Positive:\", negative)\n",
        "print(\"Num. of False Positive:\", fp)\n",
        "print(\"Num. of True Negative:\", tn)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Compute Accuracy, Precision, Recall, and F-score\n",
        "accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp + fn)\n",
        "fscore = 2*precision*recall/(precision + recall)\n",
        "print(\"Accuracy: %.2f\" % accuracy)\n",
        "print(\"Precision: %.2f\" % precision)\n",
        "print(\"Recall: %.2f\" % recall)\n",
        "print(\"F-score: %.2f\" % fscore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNX8WVRdeoX2"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 4\n",
        "\n",
        "Given training and predicted data for 4-class classification, where classes are 0,1,2,3,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IksKxswjeoX2"
      },
      "outputs": [],
      "source": [
        "y_true = np.array([1, 2, 3, 0, 1, 1, 0, 0, 2, 0, 2, 2, 1, 0, 1, 2, 3, 3, 3, 3, 0, 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 3, 2, 1, 1, 2, 1, 0, 3, 1, 2, 0, 0, 1,3, 2, 3, 3, 3, 0, 3, 1, 0, 1, 0, 2, 0, 3, 3, 3, 2, 0, 1, 3, 0, 1,3, 3, 2, 2, 1, 3, 2, 2, 2, 2, 3, 1, 1, 2, 3, 2, 2, 3, 2, 2, 1, 1,2, 2, 0, 3, 0, 0, 0, 1, 0, 3, 1, 0, 0, 3, 3, 1, 0, 2, 0, 3, 3, 2,1, 0, 3, 2, 0, 1, 3, 1, 2, 1, 3, 2, 2, 2, 2, 3, 3, 3, 2, 0, 2, 2,2, 0, 2, 0, 0, 3, 0, 2, 2, 0, 1, 1, 2, 2, 3, 0, 1, 0, 1, 0, 0, 0,0, 0, 2, 2, 0, 3, 3, 2, 3, 0, 3, 3, 2, 0, 0, 1, 2, 2, 3, 3, 1, 3,1, 2, 1, 2, 0, 0, 0, 0, 2, 2, 1, 3, 0, 2, 1, 1, 1, 1, 3, 2, 1, 3,1, 0])\n",
        "y_pred = np.array([1, 1, 1, 1, 2, 1, 0, 0, 2, 0, 2, 2, 1, 3, 1, 2, 1, 3, 2, 3, 1, 1, 2, 3, 0, 0, 1, 0, 1, 0, 2, 3, 2, 0, 1, 3, 1, 0, 0, 1, 3, 1, 0, 1,3, 3, 3, 3, 3, 1, 3, 1, 2, 2, 0, 2, 0, 3, 3, 3, 0, 2, 2, 3, 2, 2,3, 3, 2, 2, 1, 3, 3, 2, 3, 2, 3, 2, 2, 0, 3, 1, 2, 3, 3, 1, 2, 2,2, 2, 0, 3, 0, 0, 0, 1, 0, 3, 1, 0, 0, 3, 3, 2, 0, 2, 0, 3, 3, 2,1, 0, 3, 2, 0, 1, 3, 1, 2, 1, 0, 3, 2, 2, 0, 3, 3, 3, 2, 1, 2, 3,2, 0, 2, 0, 0, 3, 0, 2, 3, 0, 1, 1, 2, 2, 0, 0, 2, 0, 1, 0, 0, 0,0, 0, 2, 2, 0, 0, 3, 2, 3, 0, 0, 3, 3, 0, 1, 1, 2, 2, 3, 3, 2, 3,1, 2, 1, 2, 1, 1, 0, 0, 2, 2, 1, 3, 0, 2, 1, 1, 1, 1, 3, 3, 1, 3,1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y32s75TpeoX2"
      },
      "source": [
        "1. Compute the confusion matrix, i.e. a 4x4 matrix $\\overline{A}$, in which an element $\\overline{A}_{i,j}$ contains the number of elements that were predicted to be in class $i$, while their true class is $j$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz451dggeoX2"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "ConfusionMatrix = np.empty((4,4), int)\n",
        "\n",
        "for i in range(4):\n",
        "    # Identify objects that are predicted to be in class i\n",
        "    pred_class_i_mask = y_pred == i\n",
        "    for j in range(4):\n",
        "        # Among those objects count those whose true class is j\n",
        "        ConfusionMatrix[i][j] = np.count_nonzero(y_true[pred_class_i_mask] == j)\n",
        "\n",
        "print(ConfusionMatrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js4GYSQXeoX2"
      },
      "source": [
        "2. Compute Precision, Recall, F-score for every class, and compute the macro F-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDNKDSVpeoX2"
      },
      "outputs": [],
      "source": [
        "precisionArray = np.empty(4, float)\n",
        "recallArray = np.empty(4, float)\n",
        "FscoreArray = np.empty(4, float)\n",
        "\n",
        "for i in range(4):\n",
        "    precisionArray[i] = ConfusionMatrix[i][i] / ConfusionMatrix[i,:].sum()\n",
        "    recallArray[i] = ConfusionMatrix[i][i] / ConfusionMatrix[:,i].sum()\n",
        "    FscoreArray[i] = 2*precisionArray[i]*recallArray[i] / (precisionArray[i] + recallArray[i])\n",
        "    print(\"Class\", str(i)+\":\\n\", \"Precision: %.2f\" % precisionArray[i], \"\\n\", \"Recall: %.2f\" % recallArray[i], \"\\n\", \"F-score: %.2f\" % FscoreArray[i], \"\\n\")\n",
        "\n",
        "macroFscore = FscoreArray.mean()\n",
        "\n",
        "print(\"Macro F-score: %.2f\" % macroFscore)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}